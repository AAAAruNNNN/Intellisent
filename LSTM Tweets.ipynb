{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('intellisent/tweets_new.csv')\n",
    "# Keeping only the neccessary columns\n",
    "data.dtypes.index\n",
    "data = data[['text', 'tweet_class']]\n",
    "\n",
    "# data.drop(data['tweet_class'] == 2 , inplace=True)\n",
    "\n",
    "#removing all Junk tweets\n",
    "# data_all = data[ data.tweet_class != 2 ]\n",
    "\n",
    "data_all = data.copy()\n",
    "\n",
    "# print(len( data[data['tweet_class'] == 1]  ))\n",
    "\n",
    "# print(len( data[data['tweet_class'] == -1]  ))\n",
    "\n",
    "# print(len( data[data['tweet_class'] == 0]  ))\n",
    "\n",
    "data = pd.DataFrame()\n",
    "\n",
    "data = data.append(data_all[data_all['tweet_class']==-1])\n",
    "data = data.append(data_all[data_all['tweet_class']==0][:125])\n",
    "data = data.append(data_all[data_all['tweet_class']==1][:125])\n",
    "data = data.append(data_all[data_all['tweet_class']==2][:125])\n",
    "\n",
    "# data[data['tweet_class'] == -1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7      me gustó un video de    why people stopped wat...\n",
      "12     i liked a  video   why people stopped watching...\n",
      "19     the walking dead gotta be the dumbest show eve...\n",
      "31     at least better than the walking dead aka 7 ep...\n",
      "43     i’m sorry but this show is a shell of the walk...\n",
      "63     series i once loved but had to bail on:\\nlost\\...\n",
      "66      hey wanna fight since u guys are negan apolog...\n",
      "78     i was dissapointed with everyone's reaction, t...\n",
      "81                    when is the walking dead gonna end\n",
      "96     i started binge watching the walking dead from...\n",
      "116    fat people in theme parks are the real walking...\n",
      "131    i cannot stress enough how much i hate the wal...\n",
      "137                  the walking dead has got so boring \n",
      "140    carl when he finds out he how many views the w...\n",
      "143    holy shit! please woman! this isn't a casting ...\n",
      "144    is it me or is the walking dead been really bo...\n",
      "147                                      i miss the old \n",
      "153     i've tried to make it work. there's been mayb...\n",
      "154    for. kill. kill .kill  from rick. i'm a hardco...\n",
      "173    will it be more boring than the other 2 past e...\n",
      "178    i liked a  video   why people stopped watching...\n",
      "193    drop everything? more of the same, i'm getting...\n",
      "210    i think i only watch the walking dead now beca...\n",
      "215    if someone on the walking dead doesn't get sup...\n",
      "219    am i the only one who hates the walking dead n...\n",
      "224     remember when the walking dead was actually good\n",
      "236    he made the right choice. walking dead has one...\n",
      "237    *confession* the walking dead is starting to g...\n",
      "240    don't worry.... someone will spill it to you a...\n",
      "252    i was a huge #thewalkingdead fan since season ...\n",
      "                             ...                        \n",
      "168    so stoked that i'm going to be presenting my r...\n",
      "169                                  the walking dead  …\n",
      "171      vient de regarder the walking dead s08e11 avec \n",
      "172      vient de regarder the walking dead s08e10 avec \n",
      "174               the walking dead , stranger things ...\n",
      "177    the walking dead. or, at least the bottoms hal...\n",
      "179    you're walking in the woods\\nthere's no one ar...\n",
      "181    that is why all dems voted for god to go to he...\n",
      "185    what games do you like to play? — oof i don’t ...\n",
      "188    watch \"fear the walking dead s4 exclusive new ...\n",
      "190                   estou assistindo the walking dead \n",
      "191    list book online new all book free | the walki...\n",
      "197    lucille is in the spirit, she's all dressed up...\n",
      "198                                       share? donate?\n",
      "199    boy i swear i was the walking dead yesterday f...\n",
      "202                               the walking dead...  …\n",
      "206    i started the game the walking dead: season tw...\n",
      "207                              is there any in the uk?\n",
      "208       \\n\\nwe are here! gonna be loud, be proud an...\n",
      "209    i liked a  video   fixing the train: part 14! ...\n",
      "211    everyone is hopping on this train to nyc in th...\n",
      "212                           j’reprend the walking dead\n",
      "213    oooo the latest video for the walking dead- th...\n",
      "214    marquei como visto the walking dead - 8x9 - ho...\n",
      "216    awful human beings.  i can’t remember where i ...\n",
      "217    where's his smile what did you do to him it's ...\n",
      "218    fear the walking dead season 4 \"i lost myself\"...\n",
      "221       i liked a  video   the walking dead: season 50\n",
      "223    and ritual 'primative superstition'. however t...\n",
      "225    absolutely! i've done my little pony, initial ...\n",
      "Name: cleaned, Length: 500, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "# data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def clean(x):\n",
    "    remove_url = re.sub(r'http\\S+', '', x)\n",
    "    remove_replies = re.sub(r'@\\S+', '', remove_url)\n",
    "    return remove_replies\n",
    "    \n",
    "data['cleaned'] = data['text'].apply(clean)\n",
    "\n",
    "print(data['cleaned'])\n",
    "\n",
    "# data[\"clean\"] = data[\"unigrams\"].apply(nltk.word_tokenize)\n",
    "\n",
    "\n",
    "for idx,row in data.iterrows():\n",
    "    row[0] = row[0].replace('rt',' ')\n",
    "    \n",
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['cleaned'].values)\n",
    "X = tokenizer.texts_to_sequences(data['cleaned'].values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 58, 200)           1800200   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 58, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               168448    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 260       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 1,977,164\n",
      "Trainable params: 1,977,164\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1], dropout=0.2))\n",
    "# model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "# model.add(Dense(4,activation='softmax'))\n",
    "# model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "model = Sequential()\n",
    "model.add(Embedding(9000 + 1, 200, input_length = X.shape[1]))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(64))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(335, 58) (335, 4)\n",
      "(165, 58) (165, 4)\n"
     ]
    }
   ],
   "source": [
    "df = data.copy()\n",
    "df.loc[df.tweet_class == -1, 'tweet_class'] = 3\n",
    "\n",
    "Y = df['tweet_class']\n",
    "\n",
    "# encoder = LabelBinarizer()\n",
    "# transformed_Y = encoder.fit_transform(Y)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "transformed_Y = to_categorical(Y)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,transformed_Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(165, 58)\n",
      "(335, 58)\n",
      "(165, 4)\n",
      "(335, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(X_train.shape)\n",
    "print(Y_test.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      " - 16s - loss: 1.4468 - acc: 0.2746\n",
      "Epoch 2/7\n",
      " - 17s - loss: 1.3896 - acc: 0.2746\n",
      "Epoch 3/7\n",
      " - 17s - loss: 1.3215 - acc: 0.4119\n",
      "Epoch 4/7\n",
      " - 18s - loss: 1.0158 - acc: 0.6030\n",
      "Epoch 5/7\n",
      " - 17s - loss: 0.5820 - acc: 0.7672\n",
      "Epoch 6/7\n",
      " - 16s - loss: 0.3531 - acc: 0.9015\n",
      "Epoch 7/7\n",
      " - 17s - loss: 0.2242 - acc: 0.9582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f379b84f748>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 7\n",
    "model.fit(X_train, Y_train, epochs = 7, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 1.61\n",
      "acc: 0.54\n"
     ]
    }
   ],
   "source": [
    "validation_size = 50\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 98, 29, 8, 165, 165, 165]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
       "         98,  29,   8, 165, 165, 165]], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_test = tokenizer.texts_to_sequences([\"the new season is bad bad bad\"])\n",
    "\n",
    "print(my_test) \n",
    "\n",
    "import numpy \n",
    "\n",
    "test1 = numpy.asarray(my_test)\n",
    "\n",
    "test1 = pad_sequences(test1, maxlen=58)\n",
    "\n",
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict_classes(test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
